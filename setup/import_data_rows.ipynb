{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import labelbox as lb\n",
    "from tqdm import tqdm\n",
    "from API_key_personal import PERSONAL_API_KEY\n",
    "from pprint import pprint\n",
    "\n",
    "# Clients\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "labelbox = lb.Client(PERSONAL_API_KEY) \n",
    "\n",
    "# Bucket where pictures are stored\n",
    "bucket_name = \"lb-mlse\"\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"Animal_ML\"\n",
    "\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "\n",
    "pages = paginator.paginate(Bucket=bucket_name, Prefix=\"animals\")\n",
    "\n",
    "\n",
    "dataset = list()\n",
    "\n",
    "for page in pages:\n",
    "    for obj in page[\"Contents\"]:\n",
    "        object_key = obj[\"Key\"]\n",
    "\n",
    "        object_url = f'https://{bucket_name}.s3.us-east-1.amazonaws.com/{object_key}'\n",
    "        #https://lb-mlse.s3.us-east-1.amazonaws.com/video/video-168\n",
    "        \n",
    "        data = dict()\n",
    "        data[\"row_data\"] = object_url\n",
    "        data[\"global_key\"] = str(uuid.uuid4())\n",
    "        data[\"media_type\"] = \"IMAGE\"\n",
    "        \n",
    "        dataset.append(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26179\n",
      "{'global_key': '03ce405a-46d3-4df5-a0c5-ae2165ca454e',\n",
      " 'media_type': 'IMAGE',\n",
      " 'row_data': 'https://lb-mlse.s3.us-east-1.amazonaws.com/animals/image-0'}\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#Import function\n",
    "def upload_data_rows_threading(assets: list[dict[str:str]], dataset: lb.Dataset, batch_size: int) -> None:\n",
    "    \"\"\"Upload a data rows to Labelbox in parallel\n",
    "\n",
    "    :param assets: List of data row objects\n",
    "    :param dataset: Labelbox dataset to upload \n",
    "    :param object_name: Amount of data rows per parallel operation\n",
    "    \"\"\"\n",
    "\n",
    "    payload = [assets[i:i+batch_size] for i in range(0, len(assets), batch_size)]\n",
    "\n",
    "    def threading_callback(assets: list[dict[str:str]]):\n",
    "        try:\n",
    "            task = dataset.create_data_rows(assets)\n",
    "            task.wait_till_done(1800)\n",
    "        \n",
    "            if task.errors:\n",
    "                print(task.errors)\n",
    "            \n",
    "            return task\n",
    "        except:\n",
    "            return task\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=3) as executer:\n",
    "        results:list[lb.Task] = list(tqdm(executer.map(threading_callback, payload), total=len(payload), colour=\"red\", desc=\"Upload Data Rows\"))\n",
    "    \n",
    "    in_progress_lst: list[lb.Task] = []\n",
    "    error_lst: list[lb.Task] = []\n",
    "    success_lst: list[lb.Task] = []\n",
    "    for result in results:\n",
    "        status = result.status\n",
    "        if status == \"IN_PROGRESS\":\n",
    "            in_progress_lst.append(result)\n",
    "        elif status == \"FAILED\":\n",
    "            error_lst.append(result)\n",
    "        else:\n",
    "            success_lst.append(result)\n",
    "\n",
    "    \n",
    "    print(f\"Errors: {len(error_lst)}\")\n",
    "    print(f\"Success: {len(success_lst)}\")\n",
    "\n",
    "    if error_lst:\n",
    "        error = []\n",
    "        for task in error_lst:\n",
    "            print(task.errors)\n",
    "            error.append(task.failed_data_rows)\n",
    "        with open(\"errors.txt\", \"a\") as file:\n",
    "            file.write(str(error))\n",
    "    elif in_progress_lst:\n",
    "        for task in in_progress_lst:\n",
    "            with open(\"./task_bug.txt\", \"a\") as file:\n",
    "                file.write(task.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload Data Rows:   0%|\u001b[31m          \u001b[0m| 0/8 [01:38<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Upload\n",
    "\n",
    "labelbox_dataset = labelbox.create_dataset(name=dataset_name)\n",
    "upload_data_rows_threading(dataset, labelbox_dataset, 3500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animal_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
